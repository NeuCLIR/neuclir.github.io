# 2024 TREC NeuCLIR Track

*Version 1.0; 24 March 2024*

NeuCLIR is back for a third year! This year, we continue the CLIR and MLIR news and technical document retrieval tasks, and we introduce a new Report Generation task.

See full guidelines below:
 - [CLIR and MLIR Tasks](https://docs.google.com/document/d/1Vy9538kPvyE3mfIhd-stqULbcRCnS_oe5rP6ykqKA-0/edit?usp=sharing)
 - [Report Generation Task](https://docs.google.com/document/d/1Q4SSwM69kfK2GtYf0N__4eQcsEK0giMky2XG-dPoCYM/edit?usp=sharing)


## CLIR and MLIR Tasks

See the [guidelines](https://docs.google.com/document/d/1Vy9538kPvyE3mfIhd-stqULbcRCnS_oe5rP6ykqKA-0/edit?usp=sharing) for full details.

### TL;DR

Participating systems will receive a Chinese, Persian, and/or Russian document collection and a set of English topic descriptions. For each topic, systems will produce a ranked list of document IDs drawn from the collection in the order of predicted relevance to the topic. MLIR entries will produce an integrated ranked list across all three languages. Technical Document entries will rank documents drawn from a set of Chinese technical abstracts. Scoring will include traditional TREC metrics such as nDCG@20.

### Summary

Cross-language Information Retrieval (CLIR) has been studied at TREC and subsequent evaluations for more than twenty years. Prior to the application of deep learning, strong statistical approaches were developed that work well across many languages. As with most other language technologies though, neural computing has led to significant performance improvements in information retrieval. Incorporation of neural advances into CLIR is now well underway.

The TREC 2024 NeuCLIR track presents a cross-language information retrieval challenge. NeuCLIR topics are written in English. NeuCLIR has three target language collections in Chinese, Persian, and Russian, and a Chinese technical documents collection. Topics are written in the traditional TREC format: a short title and a sentence-length description. Systems are to return a ranked list of documents for each topic. Results will be pooled, and systems will be evaluated on a range of metrics. New in 2024 is a pilot report generation task, in which systems will receive a report request and must search the document collection from which to automatically write a report. The guidelines for the pilot appear in Pilot Report Generation Task Guidelines.


| Task                                | Document Language(s) | Query Language            |
| ----------------------------------------- | -------------------- | ------------------------- |
| Single Language News                      | fas                  | eng, fas, other           |
| Single Language News                      | rus                  | eng, rus, other           |
| Single Language News                      | zho                  | eng, zho, other           |
| Multilingual News                         | fas+rus+zho          | eng, fas, rus, zho, other |
| Single-Language Technical                 | zho                  | eng, zho, other           |

-------

## Report Generation (Pilot Task)

See the [guidelines](https://docs.google.com/document/d/1Q4SSwM69kfK2GtYf0N__4eQcsEK0giMky2XG-dPoCYM/edit?usp=sharing) for full details.

### TL;DR
Participating systems will receive a Chinese, Persian, or Russian document collection and a request for a report to be written in English. They will produce a textual report that fulfills the request in which each sentence points to up to two documents that support it. Scoring will be based on the percentage of main points the report successfully includes, and the precision of the report sentences in describing those main points.

Examples of [report request](https://neuclir.github.io/assets/data/report-generation-sample.request.zh.jsonl) and [evaluation data](https://neuclir.github.io/assets/data/report-generation-sample.evaluation.zh.jsonl) are available as development data.

### Summary

NeuCLIR 2024 is introducing a new Retrieval Augmented Generation (RAG) task of producing a report in one language based on the retrieval of documents in another language. The goals of the task are to stimulate research in cross-language RAG, to identify the best current approaches to automatic cross-language report-writing, to produce a collection that is useful for evaluating future systems, and to examine what parts of this evaluation might be automated to allow future systems to be scored using the same evaluation data. This year report generation is a pilot task, so participants might expect some details of the task to change slightly over time.

The report generation task is to automatically generate a report based on an English description of the desired report and a document collection in Chinese, Persian, or Russian. Generated reports must be responsive to the statements of the information need. A valid report will cite source documents that contain the information discussed within the report. Citations are one of the key attributes of this task. A report length limit will encourage systems to express information succinctly. The generated report must be written in the language of the report request (English) rather than in the language of the documents. Submitted reports created by report-writing systems will be evaluated by assessors. 

| Task                                | Document Language(s) | Query Language            |
| ----------------------------------------- | -------------------- | ------------------------- |
| Report Generation | fas                  | eng                       |
| Report Generation | rus                  | eng                       |
| Report Generation | zho                  | eng                       |



-------

## Important Dates

 - March 2022:		Evaluation document collection released
 - March 25, 2024:  	Track guidelines released
 - June, 2024:	  	CLIR/MLIR: Topics released and Report requests released
 - July, 2024: 		Technical Document Topic Release
 - August 6, 2024:  	Submissions due to NIST
 - October, 2024: 	Results distributed to participants
 - November 2024:	TREC 2024

-------

## Changes from 2023

- The technical documents pilot has been promoted to a full task.
- Later submission deadline!
- A pilot of a query-driven report generation task (Generative IR) from the multilingual document set.

-------

## Organizers

In alphabetical order:

- [Dawn Lawrie](https://hltcoe.jhu.edu/researcher/dawn-lawrie/), Johns Hopkins University, HLTCOE
- [Sean MacAvaney](https://macavaney.us/), University of Glasgow
- [James Mayfield](https://hltcoe.jhu.edu/researcher/james-mayfield/), Johns Hopkins University, HLTCOE
- [Paul McNamee](https://pmcnamee.net/), Johns Hopkins University, HLTCOE
- [Douglas W. Oard](https://ischool.umd.edu/about/directory/douglas-w-oard), University of Maryland
- [Luca Soldaini](https://soldaini.net), Allen Institute for AI
- [Eugene Yang](https://www.eugene.zone/), Johns Hopkins University, HLTCOE

<span class='navigate_toc'><i class="fas fa-arrow-up right-margin"></i><a href='#' class='navigate_toc'>Back to top</a></span>

-------

## Contact

- We encourage all interested researchers to join our [mailing list](https://groups.google.com/g/neuclir-participants) for the latest announcement and news.
- For any questions, please reach out to the organizers at [neuclir-organizers@googlegroups.com](mailto:neuclir-organizers@googlegroups.com).
- Follow us on Twitter at <a href='https://twitter.com/neuclir' title='link to '><i aria-hidden="true" class="fab fa-twitter">@neuclir</i></a> to connect with other NeuCLIR participants.

<span class='navigate_toc'><i class="fas fa-arrow-up right-margin"></i><a href='#' class='navigate_toc'>Back to top</a></span>
