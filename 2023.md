# 2023 TREC NeuCLIR Track


<div class='register-banner'>
    <div class='register-banner-items'>
    <b> This page is currently under construction. </b> Please check back later for updates.
    <!-- <b>Access</b> to 2023 NeurCLIR datasets (documents and queries) require <b>registration to TREC</b>, which is <b>free</b> and can be started from <a href='https://ir.nist.gov/trecsubmit.open/application.html'>this webpage</a>. -->
    </div>
</div>

Cross-language Information Retrieval (CLIR) has been studied at TREC and subsequent evaluations for more than twenty years.
Prior to the application of deep learning, strong statistical approaches were developed that work well across many languages.
As with most other language technologies though, neural computing has led to significant performance improvements in information retrieval.
CLIR has just begun to incorporate neural advances.

The TREC 2023 NeuCLIR track presents a cross-language information retrieval challenge.
NeuCLIR topics are written in English.
NeuCLIR has three target language collections in **Chinese**, **Persian**, and **Russian**.
Topics are written in the traditional TREC format: a short title and a sentence-length description.
Systems are to return a ranked list of documents for each topic.
Results will be pooled, and systems will be evaluated on a range of metrics.

#### Jump to:

- [Tasks](#tasks)
- [Registration](#registration)
- [Timeline](#timeline)
- [Organizers](#organizers)

-------

## Tasks

- For a detailed description of the tasks, please see the [NeuCLIR 2022 Guidelines](/2022).

- For an overview of the changes to Neuclir 2023, please see the [NeuCLIR 2023 Planning slides](https://docs.google.com/presentation/d/1wuo7zzxS5qRtZfnzsERK3p3Y5KaYVWwyWE-ClvI7hWs/edit#slide=id.g17bc251da10_0_220).

<!-- ### Ad Hoc CLIR Task

The main task in the NeuCLIR track is ad hoc cross-language information retrieval.
Systems will receive a document collection in Chinese, Persian, or Russian, and a set of English topics.
For each topic, the system will return a ranked list of 1000 documents drawn from the document collection, ordered by likelihood of relevance to the topic.

### Reranking CLIR Task

The reranking task is an extension of the ad hoc task.
In addition to a document collection and a set of topics, systems also receive as input for each topic a ranked list of 1000 documents drawn from the document collection.
Each ranked list is the output of a retrieval system operating on the ad hoc task.
Reranking task results must be drawn only from the documents that appear in these lists.
In all other ways, the reranking task is identical to the ad hoc task. We will release runs for reranking for the HC4 training topics.

### Monolingual Retrieval Task

While monolingual retrieval is not a focus of the NeuCLIR track, monolingual runs can improve assessment pools, and serve as good points of reference for cross-language runs.
The monolingual retrieval task is identical to the ad hoc task, but uses topic files that are human translations of the English topics into the target language in a way that would be expressed by speakers of the language. -->

<span class='navigate_toc'><i class="fas fa-arrow-up right-margin"></i><a href='#' class='navigate_toc'>Back to top</a></span>

-------

## Registration

<!-- - Sign up with NIST, using the link at the bottom of the [Call for Participation](https://trec.nist.gov/pubs/call2023.html).  That’s when you choose a team name.  If you don’t do this, your team name won’t be in the submission system, and you won’t be able to submit runs.  This also gets you one email address on the NIST mailing list, which is where all the NIST coordination information will be sent. -->
- The sign up page to register for TREC 2023 is not available yet.  We will update this page when it is.
- In the meantime. [Sign up](https://groups.google.com/g/neuclir-participants) for our track's Google group.  This is where we discuss things like the track guidelines.

<span class='navigate_toc'><i class="fas fa-arrow-up right-margin"></i><a href='#' class='navigate_toc'>Back to top</a></span>

-------

## Timeline

- <div id="past"><i>November 2022</i>: The 2023 edition of the NeuCLIR track is first discussed at TREC 2022; feedback from potential participants is welcome.</div>
- *May 2023*: Topic development complete.
- *June 2023*: Release test topics and baseline runs for re-ranking.
- *July 2023*: Submissions due to NIST
- *October 2023*: Distribute results to participants.
- *November 2023*: TREC 2022.

<span class='navigate_toc'><i class="fas fa-arrow-up right-margin"></i><a href='#' class='navigate_toc'>Back to top</a></span>

------

## Organizers

- [Dawn Lawrie](https://hltcoe.jhu.edu/researcher/dawn-lawrie/), Johns Hopkins University, HLTCOE
- [Sean MacAvaney](https://macavaney.us/), University of Glasgow
- [James Mayfield](https://hltcoe.jhu.edu/researcher/james-mayfield/), Johns Hopkins University, HLTCOE
- [Paul McNamee](https://pmcnamee.net/), Johns Hopkins University, HLTCOE
- [Douglas W. Oard](https://ischool.umd.edu/about/directory/douglas-w-oard), University of Maryland
- [Luca Soldaini](https://soldaini.net), Allen Institute for AI
- [Eugene Yang](https://www.eugene.zone/), Johns Hopkins University, HLTCOE

<span class='navigate_toc'><i class="fas fa-arrow-up right-margin"></i><a href='#' class='navigate_toc'>Back to top</a></span>

--------

## Contact

- We encourage all interested researchers to join our [mailing list](https://groups.google.com/g/neuclir-participants) for the latest announcement and news.
- For any questions, please reach out to the organizers at [neuclir-organizers@googlegroups.com](mailto:neuclir-organizers@googlegroups.com).
- Follow us on Twitter at <a href='https://twitter.com/neuclir' title='link to '><i aria-hidden="true" class="fab fa-twitter">@neuclir</i></a> to connect with other NeuCLIR participants.

<span class='navigate_toc'><i class="fas fa-arrow-up right-margin"></i><a href='#' class='navigate_toc'>Back to top</a></span>
